# ğŸ§  Deep Learning Basics: ANN + Backpropagation + Gradient Descent + Regularization

This repository covers foundational deep learning concepts through three core projects and hands-on code using only Python, NumPy, and scikit-learn.

---

## ğŸ“š Topics Covered

| **Concept**                         | **Definition**                                                                                                                                                      |
| ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **ANN (Artificial Neural Network)** | A model inspired by the human brain, composed of layers of interconnected nodes (neurons). It learns patterns from data to make predictions or classifications.     |
| â””â”€ **Neurons**                      | The basic computing unit that processes input data using weights and biases.                                                                                        |
| â””â”€ **Activation Functions**         | Introduce non-linearity (e.g., ReLU, Sigmoid, Tanh), enabling neural networks to learn complex functions.                                                           |
| **Feedforward Neural Network**      | A neural network where data flows in one direction: input â†’ hidden layer(s) â†’ output. No loops or recursion.                                                        |
| **MLP (Multilayer Perceptron)**     | A feedforward ANN with one or more hidden layers, commonly used in classification and regression tasks.                                                             |
| **Backpropagation**                | Training algorithm that computes gradients of the loss function with respect to weights using the chain rule. It updates weights via gradient descent.              |
| **Gradient Descent**                | Optimization algorithm to minimize a loss function by iteratively updating weights in the opposite direction of the gradient.                                       |

---

## ğŸ“ Project 1: Digit Recognition using Feedforward Neural Network

### ğŸ“Š Dataset
- `sklearn.datasets.load_digits()`
- 1,797 grayscale images (8Ã—8 pixels) of digits (0â€“9)
- Features: 64 pixels per image

### ğŸ”§ Features
- Preprocessing and normalization
- Model training using `MLPClassifier` from scikit-learn
- Accuracy evaluation
- Result visualization

### ğŸ“¦ Tech Stack
- Python
- scikit-learn
- Matplotlib

---

## ğŸ“ Project 2: Manual Backpropagation (Regression)

### ğŸ’¼ Problem
Predict salary (LPA) based on **CGPA** and **Profile Score** using a manually coded neural network in NumPy.

| CGPA | Profile_Score | LPA |
|------|----------------|-----|
| 8    | 8              | 4   |
| 7    | 9              | 5   |
| 6    | 10             | 6   |
| 5    | 12             | 7   |

### âš™ï¸ Architecture
- Input: 2 neurons
- Hidden Layer: 2 neurons (no activation)
- Output: 1 neuron (linear output)

### ğŸ” Training Logic
- Manual forward pass (dot products)
- Mean Squared Error loss
- Backpropagation to compute gradients
- Weight updates via Gradient Descent (coded from scratch)

---

## ğŸ“ Project 3: Gradient Descent Explained & Implemented

### ğŸ¯ Goal
Understand how gradient descent works and how to implement it from scratch and with `scikit-learn`.

### âœ… Covered Topics
- Cost Function: Mean Squared Error (MSE)
- Derivatives of weights and bias
- Manual gradient computation
- Weight updates over epochs

### ğŸ’» Implementations
- âœ… Pure Python & NumPy
- âœ… `SGDRegressor` from `sklearn` with scaling and reverse-transform logic

### ğŸ” Key Insight
> Feature scaling affects the learned parameters. To interpret them in the original input space, reverse the scaling:
> - \( w_{\text{orig}} = \frac{w}{\text{std}} \)
> - \( b_{\text{orig}} = b - \frac{w \cdot \mu}{\text{std}} \)

---

## ğŸ“ What You'll Learn

- How neurons work internally
- How networks learn through forward and backward passes
- Gradient descent â€” the core training loop
- Visual intuition behind optimization
- Practical and mathematical understanding of neural network training

---

## ğŸ§  Ideal For
- Beginners in Deep Learning
- Students wanting hands-on + mathematical insight
- Anyone curious how learning happens inside a neural network

---

## ğŸ§‘â€ğŸ’» Author

Built with â¤ï¸ for self-paced learning by **Me**

---

## ğŸ™Œ Happy Learning!

